{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "collapsed": false,
        "id": "e658b19c983664c"
      },
      "id": "e658b19c983664c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data extraction\n",
        "\n",
        "In this step, raw data was extracted from the BigQuery public dataset.\n",
        "10,000 rows from one day of data was extracted as a sample, to test out our model.\n",
        "\n",
        "```\n",
        "SELECT fullVisitorId, visitNumber, h.eCommerceAction.action_type, prod.productSKU, h.time, h.hitNumber\n",
        "\n",
        "FROM bigquery-public-data.google_analytics_sample.ga_sessions_20170801, UNNEST(hits) as h, UNNEST(h.product) as prod\n",
        "\n",
        "Where h.eCommerceAction.action_type != '0'\n",
        "\n",
        "order by fullVisitorId asc, visitNumber asc, h.time asc\n",
        "\n",
        "LIMIT 10000\n",
        "```\n",
        "\n",
        "\n",
        "After running the above query, we got the following set as a result, which we then exported as a CSV:\n",
        "\n",
        "```\n",
        "fullVisitorId, visitNumber, action_type, productSKU, time, hitNumber\n",
        "0049931492016965831,1,1,GGOEGEVA022399,96360,6\n",
        "0049931492016965831,1,2,GGOEGEVA022399,96361,7\n",
        "0049931492016965831,1,1,GGOEGEVA022399,106182,8\n",
        "\n",
        "```\n",
        "\n",
        "Some data preprocessing was done in the SQl part, before the data was extracted. For example, whenever the action_type was 0 (unknown action) we filtered it out. Since we cannot really extract any information from that. In addition, the result was sorted in ascending order, so as to avoid sorting during the next steps.\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d4c1a6fbee988d1d"
      },
      "id": "d4c1a6fbee988d1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Dependencies"
      ],
      "metadata": {
        "id": "nwcrbFDbOCkA"
      },
      "id": "nwcrbFDbOCkA"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import concat\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import lead\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import sum"
      ],
      "metadata": {
        "id": "KebAwefyNhol"
      },
      "id": "KebAwefyNhol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "aGFZkCb0OMX6"
      },
      "id": "aGFZkCb0OMX6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a spark session.\n",
        "def init_spark():\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Python Spark SQL basic example\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "# Useful functions to print RDDs and Dataframes.\n",
        "def toCSVLineRDD(rdd):\n",
        "    '''\n",
        "    This function convert an RDD or a DataFrame into a CSV string\n",
        "    '''\n",
        "    a = rdd.map(lambda row: \",\".join([str(elt) for elt in row]))\\\n",
        "           .reduce(lambda x, y: '\\n'.join([x,y]))\n",
        "    return a + '\\n'\n",
        "\n",
        "def toCSVLine(data):\n",
        "    '''\n",
        "    Convert an RDD or a DataFrame into a CSV string\n",
        "    '''\n",
        "    if isinstance(data, RDD):\n",
        "        return toCSVLineRDD(data)\n",
        "    elif isinstance(data, DataFrame):\n",
        "        return toCSVLineRDD(data.rdd)\n",
        "    return None"
      ],
      "metadata": {
        "id": "mx9FXgaAOO99"
      },
      "id": "mx9FXgaAOO99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function To Calculate The Session Duration\n",
        "\n",
        "This function calculates the time spent on product-detail pages. It does so by subtracting the time once the product was clicked on from time of the next page."
      ],
      "metadata": {
        "id": "MKooYVpWOXN9"
      },
      "id": "MKooYVpWOXN9"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_session_duration(filename):\n",
        "\n",
        "    # Start a Spark session\n",
        "    spark = init_spark()\n",
        "\n",
        "    # Read data from file\n",
        "    df = spark.read.csv(filename, header=True, inferSchema=True)\n",
        "\n",
        "    # Create 'userId' by concatenating 'fullVisitorID' and 'visitNumber', separated by '-'\n",
        "    df = df.withColumn(\"userId\", concat(col(\"fullVisitorID\"), lit(\"-\"), col(\"visitNumber\")))\n",
        "\n",
        "    # Define window spec for ordering by time within each user session\n",
        "    windowSpec = Window.partitionBy(\"userId\").orderBy(\"time\")\n",
        "\n",
        "    # Calculate the difference in time for consecutive hits to get the pageview duration\n",
        "    # First create a new column called next_hit_time, then initialize it with the the time of the next hit, from the next row, given it has the same userId\n",
        "    # Then obtain the session duration from doing next_hit_time - time\n",
        "    df = df.withColumn(\"next_hit_time\", lead(\"time\", 1).over(windowSpec))\n",
        "    df = df.withColumn(\"pageview_duration\", when(col(\"next_hit_time\").isNull(), 0).otherwise(col(\"next_hit_time\") - col(\"time\")))\n",
        "\n",
        "    # Filter for product detail views (where eCommerceAction.action_type == \"2\")\n",
        "    prod_view_df = df.filter(df['action_type'] == '2')\n",
        "\n",
        "    # Aggregate session durations by userId and productSKU, summing up durations to get total session duration\n",
        "    aggregate_web_stats = prod_view_df.groupBy(\"userId\", \"productSKU\").agg(sum(\"pageview_duration\").alias(\"session_duration\"))\n",
        "\n",
        "    # Rename column to itemId\n",
        "    aggregate_web_stats = aggregate_web_stats.withColumnRenamed(\"productSKU\", \"itemId\")\n",
        "\n",
        "    # Show the a sample of the result\n",
        "    aggregate_web_stats.show(10)\n",
        "\n",
        "    # write data to disk\n",
        "    aggregate_web_stats.write.csv('../data/data_v1.csv', header=True, mode='overwrite')"
      ],
      "metadata": {
        "id": "VVjNQlUBOi28"
      },
      "id": "VVjNQlUBOi28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ouput: Utility Matrix\n",
        "\n",
        "```\n",
        "userId,itemId,session_duration\n",
        "1005829299685984449-1,GGOEYFKQ020699,58012\n",
        "1005829299685984449-1,GGOEGGCX056299,787\n",
        "```"
      ],
      "metadata": {
        "id": "Oiml82IIQjkD"
      },
      "id": "Oiml82IIQjkD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}